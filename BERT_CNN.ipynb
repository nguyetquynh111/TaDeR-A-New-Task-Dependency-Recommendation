{"cells":[{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-fNeZyH94wc6","outputId":"b29711e1-22d1-4144-98e4-fcf8131fd380","executionInfo":{"status":"ok","timestamp":1707311899247,"user_tz":-420,"elapsed":2915,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MNRpmeceelg0","outputId":"7e11e04b-195c-40f3-ef74-7d7f6179d021","executionInfo":{"status":"ok","timestamp":1707311904039,"user_tz":-420,"elapsed":4795,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"]}]},{"cell_type":"markdown","metadata":{"id":"K2qpbI_cL5Tk"},"source":["# Global"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wCM6fhzSCEfp","outputId":"13781b6c-c167-471e-de43-a1d0479b9855","executionInfo":{"status":"ok","timestamp":1707311904040,"user_tz":-420,"elapsed":15,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1Hgps4QuC_8w15htjlDJJFgm2WyvBtk5Y/IssuesManagement\n"]}],"source":["cd /content/drive/My Drive/IssuesManagement"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"amGQs771MNB6","outputId":"044968c4-d03d-4bf0-d4d8-31b5f51139ae","executionInfo":{"status":"ok","timestamp":1707311904040,"user_tz":-420,"elapsed":12,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import json\n","from tqdm import tqdm\n","import tensorflow as tf\n","import torch\n","import nltk\n","nltk.download('punkt')\n","from transformers import BertTokenizer, BertModel"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"hrSCpBx6MTh3","executionInfo":{"status":"ok","timestamp":1707311904040,"user_tz":-420,"elapsed":10,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"outputs":[],"source":["PROJECT_NAME = \"FLUME\"\n","\n","TEXT_FEATURES = [\"title\", \"description\", \"summary\"]\n","ADDING_TIME_FEATURES = ['CC', 'CU']\n","\n","MAXLEN = 256\n","MODEL_NAME = \"CNN\"\n","EMBEDDING_METHOD = \"BERT\"\n","DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"code","execution_count":37,"metadata":{"id":"QoDZjSzDRCb0","executionInfo":{"status":"ok","timestamp":1707311905946,"user_tz":-420,"elapsed":1915,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"outputs":[],"source":["BERT_TOKENIZER = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","BERT_MODEL = BertModel.from_pretrained(\"bert-base-uncased\")\n","EMBEDDING_MATRIX = BERT_MODEL.embeddings.word_embeddings.weight\n","HIDDEN_SIZE = EMBEDDING_MATRIX.shape[1]\n","\n","def bert_tokenizer(sentences):\n","  X = BERT_TOKENIZER(list(sentences), truncation=True, return_tensors='pt',padding='max_length', max_length=MAXLEN)\n","  return X.get('input_ids').numpy()"]},{"cell_type":"markdown","metadata":{"id":"JZ3kJVMXL8Mv"},"source":["# Load data"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"5iSvU8LKMERd","executionInfo":{"status":"ok","timestamp":1707311906977,"user_tz":-420,"elapsed":1036,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"outputs":[],"source":["train_links = pd.read_csv(\n","    f\"data/{PROJECT_NAME}/train_links.csv\", keep_default_na=False)\n","train_links['label'] = train_links['label'].map(lambda x: int(x!=0))\n","issues = pd.read_csv(\n","    f\"data/{PROJECT_NAME}/preprocessed_attributes.csv\", index_col=\"key\")\n","issues['created'] = pd.to_datetime(issues['created'], utc=True)\n","issues['updated'] = pd.to_datetime(issues['updated'], utc=True)\n","issues = issues.fillna(\" \")\n","first_feature = TEXT_FEATURES[0]\n","issues[\"text\"] = issues[first_feature]\n","if len(TEXT_FEATURES)>1:\n","  for feature in TEXT_FEATURES[1:]:\n","    issues[\"text\"] = issues[\"text\"] + \" \" + issues[feature]"]},{"cell_type":"markdown","source":["# Filter model"],"metadata":{"id":"I1toKf3gJcAq"}},{"cell_type":"code","source":["test_links = pd.read_csv(\n","    f\"data/{PROJECT_NAME}/test_links.csv\", keep_default_na=False)\n","test_links['label'] = test_links['label'].map(lambda x: int(x!=0))"],"metadata":{"id":"x7FOP4WVpf--","executionInfo":{"status":"ok","timestamp":1707311907759,"user_tz":-420,"elapsed":785,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["def filter_data(issues, links):\n","  issues_1 = links[\"key_1\"].values\n","  issues_2 = links[\"key_2\"].values\n","  cre_1 = issues.loc[issues_1]['created'].values\n","  cre_2 = issues.loc[issues_2]['created'].values\n","  links['date_gap'] = np.abs(np.array(\n","        (cre_1-cre_2) / np.timedelta64(1, 'D'), dtype=np.float64).reshape(-1, 1))\n","  return links[links['date_gap']<=30]"],"metadata":{"id":"Vvk-cZFYnb1F","executionInfo":{"status":"ok","timestamp":1707311907760,"user_tz":-420,"elapsed":4,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["train_links = filter_data(issues, train_links)\n","test_links = filter_data(issues, test_links)"],"metadata":{"id":"a3QRYjhLn0RT","executionInfo":{"status":"ok","timestamp":1707311908211,"user_tz":-420,"elapsed":454,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"execution_count":41,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZOeo20l9L9lx"},"source":["#Training"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"l4qyIUk1NeRU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e0bcd9c4-9568-44ec-ea48-fc9042ddee76","executionInfo":{"status":"ok","timestamp":1707311908211,"user_tz":-420,"elapsed":5,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:151: UserWarning: Field \"model_name\" has conflict with protected namespace \"model_\".\n","\n","You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n","  warnings.warn(\n"]}],"source":["from pydantic import BaseModel\n","\n","class ModelConfig(BaseModel):\n","  generate_batch_size: int\n","  mul: int\n","  adding_time_features: list\n","  mean_time_features: float\n","  std_time_features: float\n","  value_maxlen: int\n","  hidden_size: int\n","  number_units: int\n","  model_name: str\n","  learning_rate: float\n","  steps_per_epoch: int\n","  epochs: int\n","\n","def generate_input(issues, train_links, config: ModelConfig):\n","    match_data = train_links[train_links[\"label\"] != 0]\n","    none_data = train_links[train_links[\"label\"] == 0]\n","    match_data = match_data[['key_1', \"key_2\"]].values\n","    none_data = none_data[['key_1', \"key_2\"]].values\n","\n","    while True:\n","        each_size = int(config.generate_batch_size/2)\n","\n","        # Shuffle index of match data\n","        shuffle_index = [index for index in np.random.choice(\n","            len(match_data), len(match_data), replace=False)]\n","        match_data = [match_data[index] for index in shuffle_index]\n","\n","        for iter in range(int(len(match_data)/each_size)):\n","            # Split data by batch size and randomly select non_match_links: 1/2 for match data, 1/2 for non-match data\n","\n","            # Get index\n","            match_index = np.array(range(iter*each_size, (iter+1)*each_size))\n","            none_index = np.array([index for index in np.random.choice(\n","                len(none_data), each_size*config.mul, replace=False)])\n","\n","            match_links = [match_data[i] for i in match_index]\n","            none_links = [none_data[i] for i in none_index]\n","\n","            # Create X by tokenizing and padding X\n","            index_pairs = np.array(match_links + none_links)\n","            text_1 = bert_tokenizer(issues[\"text\"].loc[index_pairs[:,0]].values)\n","            text_2 = bert_tokenizer(issues[\"text\"].loc[index_pairs[:,1]].values)\n","\n","            # Create label y\n","            match_y = np.vstack(\n","                [np.zeros(len(match_links)), np.ones(len(match_links))]).T\n","            none_y = np.vstack([np.ones(len(none_links)), np.zeros(len(none_links))]).T\n","            y = np.concatenate([match_y, none_y])\n","\n","            cre_1 = issues[\"created\"].loc[index_pairs[:, 0]].values\n","            cre_2 = issues[\"created\"].loc[index_pairs[:, 1]].values\n","            update = issues[\"updated\"].loc[index_pairs[:, 1]].values\n","            cre_cre = np.array(\n","                (cre_1-cre_2) / np.timedelta64(1, 'D'), dtype=np.float64).reshape(-1, 1)\n","            cre_up =  np.array(\n","                (cre_1-update) / np.timedelta64(1, 'D'), dtype=np.float64).reshape(-1, 1)\n","            if len(config.adding_time_features) == 2:\n","                time_features = np.array(\n","                    [[cre_cre[i][0], cre_up[i][0]] for i in range(len(cre_cre))])\n","            elif \"CC\" in config.adding_time_features:\n","                time_features = cre_cre\n","            elif \"CU\" in config.adding_time_features:\n","                time_features = cre_up\n","            index = np.random.choice(len(index_pairs), config.generate_batch_size, replace=False)\n","            if len(config.adding_time_features) == 0:\n","                yield [text_1[index], text_2[index]], y[index]\n","            else:\n","                time_features = np.array(\n","                [(time_features[i]-config.mean_time_features)/config.std_time_features for i in range(len(time_features))])\n","                yield [text_1[index], text_2[index], time_features[index]], y[index]\n","    return 0\n","\n","def return_model(config: ModelConfig):\n","  inputs_A = tf.keras.Input(shape=(config.value_maxlen), name=\"input_a\")\n","  inputs_B = tf.keras.Input(shape=(config.value_maxlen), name=\"input_b\")\n","\n","  embedding_layer = tf.keras.layers.Embedding(input_dim=EMBEDDING_MATRIX.shape[0],\n","                 output_dim=EMBEDDING_MATRIX.shape[1],\n","                  embeddings_initializer = tf.keras.initializers.Constant(value=EMBEDDING_MATRIX),\n","                 mask_zero=True)\n","  embedding_layer.trainable=False\n","  # Embedding\n","  emb_A = embedding_layer(inputs_A)\n","  emb_B = embedding_layer(inputs_B)\n","\n","  if len(ADDING_TIME_FEATURES)==1:\n","    inputs_C = tf.keras.Input(shape=(1), name=\"input_c\")\n","  elif len(ADDING_TIME_FEATURES)==2:\n","    inputs_C = tf.keras.Input(shape=(2), name=\"input_c\")\n","\n","  # Deep Learning model's structure\n","  flatten_layer = tf.keras.layers.Flatten(name=\"flatten\")\n","  dense_1_layer = tf.keras.layers.Dense(config.number_units, activation=\"relu\", name=\"dense_1\")\n","  output_layer = tf.keras.layers.Dense(2, activation=\"softmax\", name=\"dense_output\")\n","\n","  if config.model_name==\"CNN\":\n","    core_layer = tf.keras.layers.Conv1D(config.number_units, 3, activation='relu')\n","  elif config.model_name==\"LSTM\":\n","    core_layer = tf.keras.layers.LSTM(config.number_units)\n","  else:\n","    core_layer = tf.keras.layers.GRU(config.number_units, name=\"gru\")\n","\n","  core_A = core_layer(emb_A)\n","  core_B = core_layer(emb_B)\n","\n","  if len(ADDING_TIME_FEATURES)==0:\n","\n","    # Concat two embedded inputs\n","    X = tf.concat([flatten_layer(core_A), flatten_layer(core_B)], axis=1)\n","\n","    dense_1_X = dense_1_layer(X)\n","\n","    outputs = output_layer(dense_1_X)\n","\n","    model = tf.keras.Model(inputs=[inputs_A, inputs_B], outputs=outputs)\n","\n","  else:\n","\n","    # Concat two embedded inputs\n","    X = tf.concat([flatten_layer(core_A), flatten_layer(core_B), inputs_C], axis=1)\n","\n","    dense_1_X = dense_1_layer(X)\n","\n","    outputs = output_layer(dense_1_X)\n","\n","    model = tf.keras.Model(inputs=[inputs_A, inputs_B, inputs_C], outputs=outputs)\n","\n","  model.compile(tf.keras.optimizers.Adam(learning_rate=config.learning_rate), loss=\"mse\", metrics=[\"categorical_accuracy\"])\n","  model.summary()\n","\n","  return model\n","\n","def train_model(issues, train_links, config):\n","  model = return_model(config)\n","  history = model.fit(generate_input(issues, train_links, config),\n","              steps_per_epoch=config.steps_per_epoch,\n","              epochs=config.epochs,\n","              shuffle=False,\n","              verbose = 1)\n","  return model"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"592ysYECtySa","executionInfo":{"status":"ok","timestamp":1707311908211,"user_tz":-420,"elapsed":4,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"outputs":[],"source":["def get_normalize_parameter(issues, train_links):\n","    index_pairs = train_links[[\"key_1\", \"key_2\"]].values\n","    cre_1 = issues[\"created\"].loc[index_pairs[:, 0]].values\n","    cre_2 = issues[\"created\"].loc[index_pairs[:, 1]].values\n","    update = issues[\"updated\"].loc[index_pairs[:, 1]].values\n","    cre_cre = np.array((cre_1-cre_2) / np.timedelta64(1,\n","                       'D'), dtype=int).reshape(-1, 1)\n","    cre_up = np.array((cre_1-update) / np.timedelta64(1, 'D'),\n","                      dtype=int).reshape(-1, 1)\n","    if len(ADDING_TIME_FEATURES) == 2:\n","        time_features = np.array([[cre_cre[i][0], cre_up[i][0]]\n","                                 for i in range(len(cre_cre))])\n","    elif \"CC\" in ADDING_TIME_FEATURES:\n","        time_features = cre_cre\n","    elif \"CU\" in ADDING_TIME_FEATURES:\n","        time_features = cre_up\n","    else:\n","      time_features = [0]\n","    mean = np.mean(time_features)\n","    std = np.std(time_features)\n","    return mean, std"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"EVSwNmsftrG9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"96b78520-824d-40e6-e9fa-c955e18b6cf5","executionInfo":{"status":"ok","timestamp":1707311908640,"user_tz":-420,"elapsed":433,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["-233.00655826558267 542.4283763595195\n"]}],"source":["MEAN_TIME_FEATURES, STD_TIME_FEATURES = get_normalize_parameter(\n","    issues, train_links)\n","print(MEAN_TIME_FEATURES, STD_TIME_FEATURES)"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"dbh8qjhgvpuZ","colab":{"base_uri":"https://localhost:8080/","height":929},"outputId":"0d9c71ec-2fb4-47ed-909f-5dfcc024f510","executionInfo":{"status":"error","timestamp":1707311910284,"user_tz":-420,"elapsed":1658,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_a (InputLayer)        [(None, 256)]                0         []                            \n","                                                                                                  \n"," input_b (InputLayer)        [(None, 256)]                0         []                            \n","                                                                                                  \n"," embedding_1 (Embedding)     (None, 256, 768)             2344089   ['input_a[0][0]',             \n","                                                          6          'input_b[0][0]']             \n","                                                                                                  \n"," conv1d_1 (Conv1D)           (None, 254, 256)             590080    ['embedding_1[0][0]',         \n","                                                                     'embedding_1[1][0]']         \n","                                                                                                  \n"," flatten (Flatten)           (None, 65024)                0         ['conv1d_1[0][0]',            \n","                                                                     'conv1d_1[1][0]']            \n","                                                                                                  \n"," input_c (InputLayer)        [(None, 2)]                  0         []                            \n","                                                                                                  \n"," tf.concat_1 (TFOpLambda)    (None, 130050)               0         ['flatten[0][0]',             \n","                                                                     'flatten[1][0]',             \n","                                                                     'input_c[0][0]']             \n","                                                                                                  \n"," dense_1 (Dense)             (None, 256)                  3329305   ['tf.concat_1[0][0]']         \n","                                                          6                                       \n","                                                                                                  \n"," dense_output (Dense)        (None, 2)                    514       ['dense_1[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 57324546 (218.68 MB)\n","Trainable params: 33883650 (129.26 MB)\n","Non-trainable params: 23440896 (89.42 MB)\n","__________________________________________________________________________________________________\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-45-98d88d08dbbe>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m   epochs = 200)\n\u001b[1;32m     14\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0missues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_links\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-42-7f6c206bfa7d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(issues, train_links, config)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0missues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_links\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m   history = model.fit(generate_input(issues, train_links, config),\n\u001b[0m\u001b[1;32m    139\u001b[0m               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         ):\n\u001b[1;32m   1744\u001b[0m             \u001b[0;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m             data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[1;32m   1746\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorExactEvalDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m         \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m         self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1293\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0;31m# Since we have to know the dtype of the python generator when we build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;31m# the dataset, we have to look at a batch to infer the structure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m         \u001b[0mpeek\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_peek_and_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m_peek_and_restore\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    947\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_peek_and_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m         \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpeek\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-42-7f6c206bfa7d>\u001b[0m in \u001b[0;36mgenerate_input\u001b[0;34m(issues, train_links, config)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mindex_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch_links\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnone_links\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mtext_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0missues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mtext_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0missues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_pairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m# Create label y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-37-9330c034bec6>\u001b[0m in \u001b[0;36mbert_tokenizer\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERT_TOKENIZER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAXLEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2796\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2797\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2798\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2799\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2800\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2882\u001b[0m                 )\n\u001b[1;32m   2883\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2884\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2885\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2886\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3073\u001b[0m         )\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   3076\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    801\u001b[0m                 \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m             \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m                 \u001b[0mtokenized_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m                 \u001b[0mtokenized_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m         \u001b[0;31m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(self, text, split_special_tokens)\u001b[0m\n\u001b[1;32m    250\u001b[0m                     \u001b[0msplit_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                     \u001b[0msplit_tokens\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordpiece_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0msplit_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordpiece_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    579\u001b[0m                     \u001b[0mis_bad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0msub_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_substr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["model_config = ModelConfig(\n","  model_name = MODEL_NAME,\n","  generate_batch_size = 128,\n","  mul = 3,\n","  adding_time_features = ADDING_TIME_FEATURES,\n","  mean_time_features = MEAN_TIME_FEATURES,\n","  std_time_features = STD_TIME_FEATURES,\n","  value_maxlen = MAXLEN,\n","  hidden_size = HIDDEN_SIZE,\n","  number_units = 256,\n","  learning_rate = 1e-3,\n","  steps_per_epoch = 5,\n","  epochs = 200)\n","with torch.no_grad():\n","  model = train_model(issues, train_links, model_config)"]},{"cell_type":"markdown","metadata":{"id":"p32YpdeKqF2L"},"source":["# Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4HaLstXTq5h-","executionInfo":{"status":"aborted","timestamp":1707311910284,"user_tz":-420,"elapsed":11,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"outputs":[],"source":["from torch.utils.data import Dataset,  DataLoader\n","class TestDataset(Dataset):\n","    def __init__(self, index_pairs, labels):\n","        self.index_pairs = index_pairs\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","      return [self.index_pairs[idx,0], self.index_pairs[idx,1]], self.labels[idx]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"STiQURltrj6D","executionInfo":{"status":"aborted","timestamp":1707311910285,"user_tz":-420,"elapsed":12,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"outputs":[],"source":["def get_predict(issues, index_pairs, config, model):\n","  text_1 = bert_tokenizer(issues[\"text\"].loc[index_pairs[0]].values)\n","  text_2 = bert_tokenizer(issues[\"text\"].loc[index_pairs[1]].values)\n","  cre_1 = issues[\"created\"].loc[index_pairs[0]].values\n","  cre_2 = issues[\"created\"].loc[index_pairs[1]].values\n","  update = issues[\"updated\"].loc[index_pairs[1]].values\n","  cre_cre = np.array(\n","      (cre_1-cre_2) / np.timedelta64(1, 'D'), dtype=np.float64).reshape(-1, 1)\n","  cre_up =  np.array(\n","      (cre_1-update) / np.timedelta64(1, 'D'), dtype=np.float64).reshape(-1, 1)\n","\n","  if len(config.adding_time_features) == 2:\n","      time_features = np.array(\n","          [[cre_cre[i][0], cre_up[i][0]] for i in range(len(cre_cre))])\n","  elif \"CC\" in config.adding_time_features:\n","      time_features = cre_cre\n","  elif \"CU\" in config.adding_time_features:\n","      time_features = cre_up\n","  else:\n","    return model([text_1, text_2])\n","  if len(config.adding_time_features) > 0:\n","    time_features = np.array(\n","        [(time_features[i]-config.mean_time_features)/config.std_time_features for i in range(len(time_features))])\n","  return model([text_1, text_2, time_features])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TthrlueEtef2","executionInfo":{"status":"aborted","timestamp":1707311910285,"user_tz":-420,"elapsed":12,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"outputs":[],"source":["labels = []\n","for i in test_links[\"label\"].values:\n","  if i==0:\n","    labels.append([1,0])\n","  else:\n","    labels.append([0,1])\n","test_data = TestDataset(test_links[[\"key_1\", \"key_2\"]].values, labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zd3wIzYLvSA1","executionInfo":{"status":"aborted","timestamp":1707311910285,"user_tz":-420,"elapsed":12,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"outputs":[],"source":["test_dataloader = DataLoader(test_data, batch_size=512)\n","len(test_dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tt8QnuAS4m9Q","executionInfo":{"status":"aborted","timestamp":1707311910285,"user_tz":-420,"elapsed":11,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"outputs":[],"source":["if not os.path.exists(f\"results_{EMBEDDING_METHOD}_{PROJECT_NAME}\"):\n","  os.mkdir(f\"results_{EMBEDDING_METHOD}_{PROJECT_NAME}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M7PmsscMwKf9","executionInfo":{"status":"aborted","timestamp":1707311910285,"user_tz":-420,"elapsed":11,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"outputs":[],"source":["for index, i in enumerate(test_dataloader):\n","  index_pairs, label = i\n","  index_pairs[0] = np.array( index_pairs[0])\n","  index_pairs[1] = np.array( index_pairs[1])\n","  proba = get_predict(issues, index_pairs, model_config, model)\n","  print(f\"{index}/{len(test_dataloader)}\")\n","  if index==0:\n","    pred_proba = proba\n","    continue\n","  pred_proba=np.concatenate([pred_proba, proba])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JGmU22-c8W0m","executionInfo":{"status":"aborted","timestamp":1707311910285,"user_tz":-420,"elapsed":11,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"outputs":[],"source":["y_s = [np.argmax(i) for i in labels]\n","pred_s = [np.argmax(i) for i in pred_proba]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M6m1nzyfFp9q","executionInfo":{"status":"aborted","timestamp":1707311910286,"user_tz":-420,"elapsed":12,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, classification_report\n","print(\"Confusion maxtrix\")\n","print(confusion_matrix(y_s, pred_s))\n","print(classification_report(y_s, pred_s, digits= 2))"]},{"cell_type":"markdown","metadata":{"id":"cg4ordLeGFEZ"},"source":["# Recommend"]},{"cell_type":"code","source":["match_test_links = test_links[test_links[\"label\"]!=0]"],"metadata":{"id":"lnbz5M1aebzr","executionInfo":{"status":"aborted","timestamp":1707311910286,"user_tz":-420,"elapsed":12,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2WoR6-iNGEiM","executionInfo":{"status":"aborted","timestamp":1707311910286,"user_tz":-420,"elapsed":12,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"outputs":[],"source":["test_issues = pd.read_csv(f\"data/{PROJECT_NAME}/test_issues.csv\", index_col=\"key\").index\n","\n","y_test = []\n","filter_test_issues = []\n","for test_issue in tqdm(test_issues):\n","  filter_links = match_test_links[(match_test_links[\"key_1\"]==test_issue)|(match_test_links[\"key_2\"]==test_issue)]\n","  if len(filter_links)>0:\n","    match_issues = set(list(filter_links[\"key_1\"].values) + list(filter_links[\"key_2\"].values))\n","    match_issues.remove(test_issue)\n","    y_test.append(list(match_issues))\n","    filter_test_issues.append(test_issue)\n","test_issues = filter_test_issues"]},{"cell_type":"code","source":["len(test_issues)"],"metadata":{"id":"A4jtwyxirGIf","executionInfo":{"status":"aborted","timestamp":1707311910286,"user_tz":-420,"elapsed":12,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VfEtpF13LxbA","executionInfo":{"status":"aborted","timestamp":1707311910286,"user_tz":-420,"elapsed":11,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"outputs":[],"source":["def get_accuracy(pred, gt):\n","\tacc = 0\n","\tfor i, item in enumerate(pred):\n","\t\tif item in gt:\n","\t\t\tacc += 1.0\n","\t\t\tbreak\n","\treturn acc\n","\n","def get_MRR(pred, gt):\n","\tmrr = 0\n","\tfor i, item in enumerate(pred):\n","\t\tif item in gt:\n","\t\t\tmrr += 1.0/(i+1)\n","\treturn mrr\n","\n","def get_precision_recall(pred, gt):\n","\tright = 0\n","\n","\tfor item in gt:\n","\t\tif item in pred: # relevant\n","\t\t\tright+=1\n","\n","\tif len(pred) == 0:\n","\t\tprecision = 0\n","\telse:\n","\t\tprecision = right/len(pred)\n","\trecall = right/len(gt)\n","\n","\treturn precision, recall\n","\n","def get_f1_score(precision, recall):\n","    if precision + recall == 0:\n","        return 0\n","    f1_score = 2 * (precision * recall) / (precision + recall)\n","    return f1_score\n","\n","def get_metrics(recommend, label):\n","\tacc = 0\n","\tmrr = 0\n","\tprecision = 0\n","\trecall = 0\n","\tf1 = 0\n","\tfor i in range(0, len(recommend)):\n","\t\tif len(label[i])!=0:\n","\t\t\tacc += get_accuracy(recommend[i], label[i])\n","\t\t\tmrr += get_MRR(recommend[i], label[i])\n","\t\t\tprecision_recall = get_precision_recall(recommend[i], label[i])\n","\t\t\tprecision += precision_recall[0]\n","\t\t\trecall += precision_recall[1]\n","\t\t\tf1 = get_f1_score(precision_recall[0], precision_recall[1])\n","\n","\tacc = acc/(len(recommend))\n","\tmrr = mrr/(len(recommend))\n","\tprecision = precision/(len(recommend))\n","\trecall = recall/(len(recommend))\n","\tf1 = f1/(len(recommend))\n","\treturn acc, mrr, precision, recall, f1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9u2yOBq3NDXJ","executionInfo":{"status":"aborted","timestamp":1707311910286,"user_tz":-420,"elapsed":11,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"outputs":[],"source":["def get_recommendation(test_issue, issues, test_links):\n","  all_issues = list(issues.index)\n","  all_issues.remove(test_issue)\n","\n","  test_links_1 = pd.DataFrame(\n","      {\"key_1\": [test_issue]*len(all_issues), \"key_2\": all_issues})\n","  test_links_1[\"link_id\"] = test_links_1[\"key_1\"] + \"-\" + test_links_1[\"key_2\"]\n","  test_links_1 = test_links[test_links[\"link_id\"].isin(test_links_1[\"link_id\"].values)]\n","  pred_proba_1 = [(pred, issue) for pred, issue in zip(test_links_1[\"proba\"].values, test_links_1[\"key_2\"].values)]\n","\n","  test_links_2 = pd.DataFrame(\n","      {\"key_1\": all_issues, \"key_2\": [test_issue]*len(all_issues)})\n","  test_links_2[\"link_id\"] = test_links_2[\"key_1\"] + \"-\" + test_links_2[\"key_2\"]\n","  test_links_2 = test_links[test_links[\"link_id\"].isin(test_links_2[\"link_id\"].values)]\n","  pred_proba_2 = [(pred, issue) for pred, issue in zip(test_links_2[\"proba\"].values, test_links_2[\"key_1\"].values)]\n","\n","  pred_proba_3 = pred_proba_1 + pred_proba_2\n","  preprocess_pred_proba = {}\n","  for pred, issue in pred_proba_3:\n","    if issue not in preprocess_pred_proba:\n","      preprocess_pred_proba[issue] = pred\n","    else:\n","      preprocess_pred_proba[issue] = max(pred, preprocess_pred_proba[issue])\n","  preprocess_pred_proba = [(pred, issue) for issue, pred in preprocess_pred_proba.items()]\n","\n","  return [pair[1] for pair in sorted(preprocess_pred_proba, reverse=True)]"]},{"cell_type":"code","source":["test_links[\"proba\"] = pred_proba[:,1]"],"metadata":{"id":"RYqLDEv-XuEW","executionInfo":{"status":"aborted","timestamp":1707311910286,"user_tz":-420,"elapsed":11,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_links[\"link_id\"] = test_links[\"key_1\"] + \"-\" + test_links[\"key_2\"]"],"metadata":{"id":"Ks0nvGBhciuJ","executionInfo":{"status":"aborted","timestamp":1707311910286,"user_tz":-420,"elapsed":11,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d029XQ_tQONE","executionInfo":{"status":"aborted","timestamp":1707311910287,"user_tz":-420,"elapsed":12,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"outputs":[],"source":["recommend_results = []\n","for test_issue in tqdm(test_issues):\n","    recommend_results.append(get_recommendation(\n","        test_issue, issues, test_links))"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"nu4ZRBciQgHD","executionInfo":{"status":"ok","timestamp":1707311916816,"user_tz":-420,"elapsed":593,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}},"outputId":"2bb660ac-97f2-40f8-e13d-e71b441eda1e","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Top 1:\n","Acc = 0.025974025974025976\n","MRR = 0.025974025974025976\n","Recall = 0.025974025974025976\n","F1 = 0.0\n","Top 2:\n","Acc = 0.05844155844155844\n","MRR = 0.04220779220779221\n","Recall = 0.054112554112554105\n","F1 = 0.004329004329004329\n","Top 3:\n","Acc = 0.05844155844155844\n","MRR = 0.04220779220779221\n","Recall = 0.054112554112554105\n","F1 = 0.003246753246753247\n","Top 5:\n","Acc = 0.11688311688311688\n","MRR = 0.05714285714285715\n","Recall = 0.1147186147186147\n","F1 = 0.002164502164502165\n","Top 10:\n","Acc = 0.2532467532467532\n","MRR = 0.07695320552463404\n","Recall = 0.25108225108225113\n","F1 = 0.001443001443001443\n"]}],"source":["for k in [1,2,3,5,10]:\n","  recommend_list = [i[:k] for i in recommend_results]\n","  acc, mrr, precision, recall, f1 = get_metrics(recommend_list, y_test)\n","  print(f\"Top {k}:\")\n","  print(f\"Acc = {acc}\")\n","  print(f\"MRR = {mrr}\")\n","  print(f\"Recall = {recall}\")\n","  print(f\"F1 = {f1}\")"]},{"cell_type":"code","source":[],"metadata":{"id":"NFfi2GJItwqr","executionInfo":{"status":"aborted","timestamp":1707311910287,"user_tz":-420,"elapsed":11,"user":{"displayName":"Quỳnh Nguyễn","userId":"09737101412508059903"}}},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}